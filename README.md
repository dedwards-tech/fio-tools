# `fio-tools`

fio is a great tool for workload modeling and performance characterization.  In a vsphere world where
workloads are generated by virtual machines, its a bit more tricky.  You either want to run multiple
VM's with specific workloads, or you want to run one VM to stress a device to its limits.  Either way,
it is tricky to geenrate fio config files for a suite of tests and various device combinations;
specifically multiple direct io devices.

For example:

1. run a block sweep by varying block size and keeping other parameters such as queue depth and jobs the same or limited.
2. run the same block sweep on 1, 2, 4, and 8 devices on a storage controller to determine boundaries of the storage controller and device set; such as HDD's versus SSD's
3. collate the ouptut into .csv or other form for graphing and simple comparison between runs.

These are some scripts I use and evolve to help with my day to day work of pushing IO's to different
storage configurations.  This gives me a baseline for what to expect from both IO controller and storage
devices as more or less devices are added.

NVMe devices can still be tested but require some finesse in order to truely load the devices with
all the IO the system can possibily throw at it.

# Design

The use of and primary design of these scripts is to assist testing in a vmware vSphere environment where
virtual machines are the primary IO generator.  I've found that a single VM can generate well into 120K IOPS
from a single CPU if the VM, driver and device are capable.  Any more than that is difficul to drive unless
the vSphere kernel, IO stack or other factors change.

`fio-gen.py`
* Use a set of more complex workload definitions to generate a set of fio scripts based on common parameters for consistency in all test data; no human error in maintaining changes.
* Generate a fio-exec.py input config that makes it very simple to execute the entire set of fio scripts and workloads in the defined sequence, or specify a subset of a set of workloads.
* Minimize python library dependencies for executing scripts and reserve those for use with generating of scripts.
* Copy a simple fio-exec.py script to the output folder so all input files and execution scripts are self contained and can be archived and delivered to the system for execution or sharing.

`fio-exec.py`
* Goal #1: Allow for re-run of workload stimulus is possible by re-running exactly the same script sets, in exactly the prescribed order with the same settings.
* Goal #2: run the script without parameters
* Goal #3: run the script with an input parameter specifying a subset of workload sequences (to save time when narrowing on a workload set)
* Use an input file, generated by fio-gen.py script to specify sequences of workloads inside of various folders that contain .fio config scripts.
* Parse the JSON output into a .csv style table of results so the can be graphed or compared to other test runs
* fio output is placed in a folder named out/<sequence-name-json>
* summary output is placed in the out/ folder under sequence-name-summary.csv

# Workload Definition (input to `fio-gen.py`)

The following is a description of each YAML section and its purpose and syntax.  Look inside the fio-gen.py script
for an example of the default configuration.

A single workload is a non-sweeping definition for one off workloads.  This is good for the simple things where you
know exactly what you want to measure.

~~~
single:
   <workload_uid>:
      block_size: "<any fio supported block size e.g. 512, 4k, 1m, etc>"
      io_depth: "<outstanding IO's per job>"
      io_type: "<Sequential, Random>>"
      num_jobs: "<total jobs i.e. threads>"
      read_pct: "<0-100 in percent of IO's to read, remainder is percent writes>"
      run_time: "<-1 for until filled, # seconds otherwise>"
   <...>
~~~

A block sweep is typically a sweep of block sizes with a select IO depth.  This implementation allows for sweeping
the IO depth and number of threads as well and that is mostly for flexibility.  The IO type, read / write ratio and
run time parameters are fixed.

~~~
block_sweep:
   <workload_uid>:
      block_size: "<comma separated list of block sizes>"
      io_depth: "<comma separated list of IO depths; recommend keeping this number beteen 1 and 4 values>"
      num_jobs: "<comma separated list of jobs>"
      io_type: "<Sequential, Random>>"
      read_pct: "<0-100 in percent of IO's to read, remainder is percent writes>"
      run_time: "<-1 for until filled, # seconds otherwise>"
   <...>
~~~

A queue depth sweep is typically a sweep of IO depth with a select block size.  I found that specifying a few
different block sizes gives better idea of block size affect on results.  It behaves identical to block_sweep
and just allows you to separate the output in its own folder.

~~~
qd_sweep:
   <workload_uid>:
      block_size: "<comma separated list of block sizes>"
      io_depth: "<comma separated list of IO depths; recommend keeping this number beteen 1 and 4 values>"
      num_jobs: "<comma separated list of jobs>"
      io_type: "<Sequential, Random>>"
      read_pct: "<0-100 in percent of IO's to read, remainder is percent writes>"
      run_time: "<-1 for until filled, # seconds otherwise>"
   <...>
~~~

A sequence is an orderd list of workload_uid names to execute in a specified order.  The goal is to allow
selection of a single sequence or to run all sequences.

~~~
sequence:
   <unique_seq_tag>: "<list of workload_uid tags in order of execution>"
   <...>
~~~

A target group is a collection of target devices (as referenced by the fio VM), to simultaneously run
workloads against.  The target groups will be sweeped through if there are more than one, or a single target
group can be specified on the commandline of fio-exec.py to limit the test scope.

~~~
target_groups:
   <target_group_id>:   "<list of comma separated device names>"
   <...>
~~~

The global section overrides various "global" settings that are difficult or don't make sense to include
in a command line parameter.  I've found that group_reporting can change the output so check the JSON output
parsing for problems if you change this from its default of "true".

The reduce_tod paraneter ties directly to the gtod_reduce setting.  I've changed the name for no other reason
than shorter variable names...

~~~
global:
   group_reporting: "true"
   reduce_tod:      "false"
~~~

# Usage

Generate scripts:

~~~
 ~ # ./fio-gen.py -o ./fio_char
 ~ # tar -zcf fio_char.tgz ./fio_char
~~~

Execute fio workloads:

~~~
 ~ # scp fio_char.tgz root@<remote_ip>:/tmp/
 ~ # ssh root@<remote_ip>
 ~ # cd /tmp
 ~ # tar -zxvf fio_char.tgz
 ~ # cd ./fio_char
 ~ # sudo ./fio-exec.py
~~~

Review output:

~~~
 ~ # ls ./out/*.csv
~~~
